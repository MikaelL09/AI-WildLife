{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm53gz3URFZP"
      },
      "source": [
        "\n",
        "# **WILDLIFE TRACKING AND BEHAVIOUR ANALYSIS**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NAqoEKJ358u"
      },
      "source": [
        "***\n",
        "# **Part A - Application Area Review**\n",
        "### **1. Project Goal**\n",
        "It is essential to understand wildlife behaviour and movement to preserve natural ecosystems and protect endangered species. Making sense of the enormous amounts of data that wildlife researchers gather from tracking devices, video traps, and acoustic sensors can be difficult and demanding. To gain important insights regarding animal behaviour, migration patterns, and habitat preferences, we need efficient ways for analysing this data. The goal of this project is to explore, implement, and evaluate established <b>Object Recognition techniques</b> and <b>Artificial Intelligence (AI)</b> based approaches to extract meaningful insights for advancing our understanding of wildlife and improving conservation efforts. This project will cover a selection of AI techniques used for object recognition in wildlife tracking and behaviour analysis to compare with one another. Finally, the most efficient AI algorithm will be implemented as a prototype.\n",
        "\n",
        "### **2. Literature Review**\n",
        "Wildlife tracking and behaviour analysis represent a critical field in ecological research, contributing to the conservation of diverse animal species. In recent years, the integration of Artificial Intelligence (AI) techniques has greatly enhanced our ability to extract valuable insights from the wealth of data collected through tracking devices, camera traps, and acoustic sensors. AI techniques such as Normalized Difference Vegetation Index (NDVI) can be helpful in inferring changes in animal populations through the number of plants in an area, thereby quantifying a given species in the area, acoustic sensors can be used to monitor animal sounds and vocalizations, while heat signature detection in thermal imagery can be used to track animal, and image recognition can be used to identify individual animals based on their unique physical characteristics such as stripes or spots. As briefly discussed, AI techniques are integrated in numerous aspects in wildlife tracking and behaviour analysis. This project will discuss on existing object recognition techniques more in-depth with a literature review. The author will also perform a literature survey and performance evaluation to decide which is the most ideal Machine Learning approach.\n",
        "\n",
        "This study introduces a novel deep learning-based facial recognition pipeline (BearID) for identifying individual brown bears (Ursus arctos) in wild populations. Traditional facial recognition methods often struggle with species that lack unique markings, such as brown bears, hindering research and conservation efforts. BearID addresses this challenge by employing a two-stage approach: first, it detects bear faces in images using a trained face detector (RetinaFace), and second, it generates embeddings for each detected face using a trained face encoder (bearembed). These embeddings are then used to identify individual bears based on a trained classifier (bearsvm). BearID was evaluated using images of 100 known American black bears (Ursus americanus) from a captive population. The face detector achieved an average precision of 97.7%, demonstrating its ability to accurately detect bear faces in a variety of poses. The bearembed encoder achieved an identification accuracy of 84% when matching new images of known individuals and 71% when assigning bears as training or testing individuals. The bearsvm classifier achieved an identification accuracy of 83.9%. This pipeline has the potential to address a broad spectrum of ecological questions that require individual identification, from fine-scale behaviour to landscape-level population assessments. It also has potential for use in conservation practice, such as identifying problem individuals in human-wildlife conflicts and evaluating intrapopulation variation in the efficacy of conservation strategies. (Clapham et al., 2020)\n",
        "\n",
        "This study explores the application of deep learning to automated bird counting for regional bird distribution mapping. A deep learning model was trained on a large dataset of bird images, demonstrating high accuracy in identifying and counting birds across diverse species and habitats. This model was then applied to analyse images from various locations within the study area, generating detailed bird distribution maps. The maps revealed significant variations in bird abundance across the region, providing valuable insights into bird population dynamics and habitat preferences. This automated bird counting method offers several advantages over traditional manual counting techniques, including its cost-effectiveness, efficiency, and ability to generate large-scale bird distribution maps. The findings underscore the potential of deep learning to revolutionize bird monitoring and conservation efforts, enabling informed decision-making for the protection of critical habitats and the persistence of bird populations. (Akçay et al., 2020)\n",
        "\n",
        "This study presents DeepBhvTracking, a novel behaviour tracking method for laboratory animals based on deep learning. Traditional animal tracking methods often rely on manual observation or specialized equipment, which can be time-consuming, labour-intensive, and expensive. DeepBhvTracking addresses these limitations by utilizing deep learning algorithms to automatically track animal movements and behaviours from video recordings. This is done by employing a two-stage approach: first, it detects and segments animals in the video frames using a YOLO model, and second, it tracks individual animals across frames using a Long Short-Term Memory (LSTM) network. The LSTM network is trained on a dataset of animal tracking annotations, enabling it to learn the temporal patterns of animal movements and maintain consistent tracking across frames. DeepBhvTracking was evaluated on a dataset of video recordings of mice and rats performing various behaviours, such as grooming, rearing, and locomotion. The results demonstrated that DeepBhvTracking achieved high accuracy in tracking animal movements and behaviours, outperforming traditional tracking methods. Overall, DeepBhvTracking represents a significant advancement in animal behaviour tracking, offering a powerful tool for researchers studying animal behaviour in a variety of laboratory settings. Its potential to automate and streamline behavioural data collection could significantly accelerate the pace of behavioural research and lead to new discoveries in animal behaviour and cognition. (Sun et al., 2021)\n",
        "\n",
        "This study introduces a large-scale, multi-class, high-quality open-source dataset (WAID - Wildlife Aerial Images from Drone) for wildlife detection with drones. It contains 14,375 UAV aerial images from different environmental conditions, covering six wildlife species and multiple habitat types. The dataset can be used to study the effectiveness of different wildlife detection algorithms and has the potential to improve the accuracy and efficiency of wildlife monitoring using drones. The paper also experimented with state-of-the-art algorithms which includes Faster R-CNN, SSD, YOLOv5, YOLOv7, and YOLOv8 on the WAID dataset. Overall, the YOLO model exhibits the best performance, with YOLOv5, YOLOv7, and YOLOv8 achieving mAP scores of 95.6%, 97.4%, and 95.8% respectively. Additionally, our proposed SE-YOLO model achieved the highest mAP of 98.3%. Although there is not a significant difference between the Faster R-CNN and SSD models overall, Faster R-CNN slightly outperforms the SSD model in accuracy while the SSD model outperforms Faster R-CNN in speed. In conclusion, this study on the small target detection problem faced by wildlife monitoring drones. To solve this problem, we propose the SE-YOLO method, which dramatically improves wildlife detection from aerial image data by integrating the SEAttention module into YOLOv7. In particular, SE-YOLO can accurately detect minimal targets with a size of 20–50 pixels. (Mou et al., 2023)\n",
        "\n",
        "\n",
        "<table>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\"><b>Citation</b></td>\n",
        "    <td style=\"width:300px;\"><b>Technologies</b></td>\n",
        "    <td style=\"width:600px;\"><b>Contribution</b></td>\n",
        "    <td style=\"width:600px;\"><b>Limitation</b></td>\n",
        "  </tr>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\">(Clapham et al., 2020)</td>\n",
        "    <td style=\"width:300px;\">Deep CNN</td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Created an open-source application called BearID that implements a novel deep learning approach for facial recognition in brown bears, a species that lacks unique markings.</li>\n",
        "          <li>Achieved an average precision of 0.98 for facial detection and an accuracy of 83.9% for individual classification.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>The study only included a limited number of individuals (132) from two populations.</li>\n",
        "          <li>The performance of the deep learning approach is affected by factors such as image quality, lighting conditions, and pose of the bear.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\">(Akçay et al., 2020)</td>\n",
        "    <td style=\"width:300px;\">Faster R-CNN and SSD</td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Developed an image-based bird population counting and mapping approach using deep learning.</li>\n",
        "          <li>Established a new dataset of wild bird photographs taken regularly in various environments.</li>\n",
        "          <li>Created spatial bird order distribution and species diversity maps of Turkey.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>The study did not consider factors such as bird behaviour and interactions, which could influence bird distribution patterns.</li>\n",
        "          <li>The study focused on a limited number of bird species (38) within Turkey.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\">(Sun et al., 2021)</td>\n",
        "    <td style=\"width:300px;\">YOLO (You Only Look Once) algorithm</td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Proposed a novel behaviour tracking method, that combines a deep learning algorithm (YOLO) and a background subtraction algorithm.</li>\n",
        "          <li>Developed a detector training workflow that combines a pretrained deep-learning neural network and YOLO.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Computationally very demanding to run.</li>\n",
        "          <li>DeepBhvTracking is not as accurate as manual tracking in situations where the animal is occluded or there are multiple animals in the scene.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\">(Mou et al., 2023)</td>\n",
        "    <td style=\"width:300px;\">Integrating the SEAttention module into YOLOv7 (SE-YOLO)</td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Provides a large-scale, multi-class, high-quality dataset for wildlife detection with drones.</li>\n",
        "          <li>Conducts an algorithm detection comparison experiment to compare and evaluate different types of advanced algorithms.</li>\n",
        "          <li>Provides a new method, data, and inspiration to the field of wildlife monitoring by UAVs.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>The dataset is limited to UAV aerial images and due to this, it is relatively small compared to other object detection datasets.</li>\n",
        "          <li>The dataset is limited to six wildlife species which means that the dataset may not be suitable for detecting other wildlife species or in other habitat types.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxa53D8u5HN-"
      },
      "source": [
        "***\n",
        "# **Part B - Comparison and Evaluation of AI Techniques**\n",
        "\n",
        "## **1.\tDeep Convolutional Neural Network**\n",
        "Deep convolutional neural networks (DCNNs) have revolutionized the field of object detection, enabling accurate and efficient identification of objects in images and videos. This capability has found significant application in wildlife tracking and behaviour analysis, providing valuable insights into animal movements, behaviour patterns, and population dynamics.\n",
        "\n",
        "DCNNs are particularly well-suited for wildlife tracking due to their ability to handle complex and varied natural scenes, often characterized by cluttered backgrounds, varying lighting conditions, and occlusions. By leveraging multiple layers of convolutional filters, DCNNs can extract intricate features from images, enabling them to distinguish between animals and their surroundings.\n",
        "Data availability for DCNN-based wildlife tracking is expanding rapidly, driven by the proliferation of camera traps and other wildlife monitoring technologies. Vast image repositories, such as WildCAM and the Microsoft COCO dataset, provide researchers with access to millions of annotated images containing a diverse range of animal species and behaviours.\n",
        "The time required to set up a DCNN-based wildlife tracking system depends on the complexity of the task and the availability of hardware resources. With pre-trained models and readily available software libraries, researchers can quickly implement a basic system for object detection and tracking. However, fine-tuning models for specific species or behaviours may require more time and expertise.\n",
        "The time to produce results from a DCNN-based wildlife tracking system varies depending on the size and complexity of the dataset. For large datasets, processing may take several hours or even days, while smaller datasets can be processed much faster. However, the output of DCNN-based systems is typically highly accurate and informative, providing detailed information on animal locations, movements, and behaviours.\n",
        "\n",
        "In summary, DCNNs have emerged as a powerful tool for object detection in wildlife tracking and behaviour analysis. Their ability to handle complex natural scenes and their increasing data availability make them well-suited for this field. While the time required for setup and processing can vary, the output of DCNN-based systems is typically highly accurate and informative, providing valuable insights into animal populations and behaviours.\n",
        "\n",
        "## **2. Faster Region-based Convolutional Neural Network**\n",
        "Faster Region-based Convolutional Neural Network (Faster R-CNN) is an advanced object detection algorithm that has proven highly effective in wildlife tracking and behaviour analysis. Its ability to generate accurate bounding boxes around objects of interest, coupled with its relatively efficient processing speed, makes it a valuable tool for analysing large volumes of wildlife data.\n",
        "\n",
        "Data availability for Faster R-CNN-based wildlife tracking is substantial, with extensive image repositories like WildCAM and the Microsoft COCO dataset providing researchers with access to millions of annotated images containing a diverse range of animal species and behaviours. This abundance of data enables researchers to train and evaluate Faster R-CNN models effectively, ensuring their accuracy and applicability to real-world wildlife monitoring scenarios.\n",
        "The time required to set up a Faster R-CNN-based wildlife tracking system depends on the researcher's familiarity with deep learning techniques and the availability of computational resources. Pre-trained models and readily available software libraries can facilitate a rapid setup, allowing researchers to focus on fine-tuning the model for their specific study objectives.\n",
        "The time to produce results from a Faster R-CNN-based wildlife tracking system varies depending on the size and complexity of the dataset. For large datasets, processing may take several hours or even days, while smaller datasets can be processed much faster. However, the output of Faster R-CNN-based systems is typically highly accurate and detailed, providing researchers with precise information on animal locations, movements, and behaviours.\n",
        "\n",
        "In summary, Faster R-CNN has emerged as a powerful tool for object detection in wildlife tracking and behaviour analysis. Its combination of accuracy, efficiency, and data availability makes it well-suited for this field. While the time required for setup and processing can vary, the output of Faster R-CNN-based systems is typically highly accurate and informative, providing valuable insights into animal populations and behaviours.\n",
        "\n",
        "## **3. Squeeze and Excitation YOLOv7**\n",
        "Squeeze and Excitation YOLOv7 (SE-YOLOv7) is a state-of-the-art object detection algorithm that has demonstrated remarkable performance in wildlife tracking and behaviour analysis. Its incorporation of the Squeeze-and-Excitation (SE) module enhances its ability to extract relevant features from complex natural scenes, leading to improved detection accuracy and efficiency.\n",
        "\n",
        "SE-YOLOv7 achieves its superior performance by employing a two-stream architecture that combines feature extraction with feature recalibration. The first stream, similar to the original YOLOv7 architecture, focuses on extracting high-resolution feature maps from the input image. The second stream, consisting of the SE module, operates on these feature maps, dynamically adjusting their weights based on their importance. This recalibration process ensures that only the most relevant features are emphasized, leading to improved object detection performance.\n",
        "The application of SE-YOLOv7 in wildlife tracking and behaviour analysis offers several advantages. First, its ability to accurately detect and localize animals in complex natural scenes, including dense foliage and low-light conditions, makes it particularly well-suited for wildlife monitoring tasks. Second, its computational efficiency allows for real-time processing of video streams, enabling researchers to monitor animal behaviour in real-time. Third, its scalability to large datasets makes it suitable for analysing vast amounts of wildlife data collected from camera traps and other monitoring technologies.\n",
        "The implementation of an SE-YOLOv7-based wildlife tracking system typically involves several steps. First, researchers need to collect and annotate a dataset of wildlife images containing the target species and behaviours of interest. Second, the SE-YOLOv7 model is trained on this dataset, optimizing its parameters to accurately detect and localize animals in the images. Third, the trained model is deployed to analyse new wildlife data, providing researchers with detailed information on animal locations, movements, and behaviours.\n",
        "The time required for each step depends on the size and complexity of the dataset, the computational resources available, and the researcher's expertise in deep learning techniques. Training a model on a large dataset can be computationally demanding and may require several hours or even days. However, pre-trained models are available, which can significantly reduce the training time. Once trained, the model can be deployed to analyse new data relatively quickly, typically within a few minutes or seconds per image or video frame.\n",
        "The output of an SE-YOLOv7-based wildlife tracking system typically consists of bounding boxes around detected animals, along with their corresponding class labels (e.g., \"elephant,\" \"zebra,\" \"bird\"). Additional information, such as animal poses and behavioural tags, may also be provided depending on the specific implementation. This detailed output enables researchers to extract valuable insights into animal populations, behaviours, and interactions with their environment.\n",
        "\n",
        "In summary, SE-YOLOv7 has emerged as a powerful tool for object detection in wildlife tracking and behaviour analysis. Its combination of accuracy, efficiency, and scalability makes it well-suited for this field. While the time required for setup and processing can vary, the output of SE-YOLOv7-based systems is typically highly accurate and informative, providing valuable insights into animal populations and behaviours.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQXk0hia5W4F"
      },
      "source": [
        "***\n",
        "# **Part C - Implementation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7vD3ny55gxN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2qSGrVv5hHN"
      },
      "source": [
        "***\n",
        "# **Part D - Testing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BktdCr215ngl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GRXKlkk5n1N"
      },
      "source": [
        "***\n",
        "# **Part E - Evaluations of Results**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD2Pgtpa5sh8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvsmGdf_423a"
      },
      "source": [
        "***\n",
        "# **References**\n",
        "\n",
        "* Akçay, H.G. et al. (2020). Automated Bird Counting with Deep Learning for Regional Bird Distribution Mapping. Animals, 10 (7), 1207. Available from https://doi.org/10.3390/ani10071207.\n",
        "\n",
        "* Clapham, M. et al. (2020). Automated facial recognition for wildlife that lack unique markings: A deep learning approach for brown bears. Ecology and Evolution, 10 (23), 12883–12892. Available from https://doi.org/10.1002/ece3.6840.\n",
        "\n",
        "* Mou, C. et al. (2023). WAID: A Large-Scale Dataset for Wildlife Detection with Drones. Applied Sciences, 13 (18), 10397. Available from https://doi.org/10.3390/app131810397.\n",
        "\n",
        "* Sun, G. et al. (2021). DeepBhvTracking: A Novel Behavior Tracking Method for Laboratory Animals Based on Deep Learning. Frontiers in Behavioral Neuroscience, 15. Available from https://www.frontiersin.org/articles/10.3389/fnbeh.2021.750894 [Accessed 20 November 2023].\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
