{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm53gz3URFZP"
      },
      "source": [
        "\n",
        "# **WILDLIFE TRACKING AND BEHAVIOUR ANALYSIS**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NAqoEKJ358u"
      },
      "source": [
        "***\n",
        "# **Part A - Application Area Review**\n",
        "### **1. Project Goal**\n",
        "It is essential to understand wildlife behaviour and movement to preserve natural ecosystems and protect endangered species. Making sense of the enormous amounts of data that wildlife researchers gather from tracking devices, video traps, and acoustic sensors can be difficult and demanding. To gain important insights regarding animal behaviour, migration patterns, and habitat preferences, we need efficient ways for analysing this data. The goal of this project is to explore, implement, and evaluate established <b>Object Recognition techniques</b> and <b>Artificial Intelligence (AI)</b> based approaches to extract meaningful insights for advancing our understanding of wildlife and improving conservation efforts. This project will cover a selection of AI techniques used for object recognition in wildlife tracking and behaviour analysis to compare with one another. Finally, the most efficient AI algorithm will be implemented as a prototype.\n",
        "\n",
        "### **2. Literature Review**\n",
        "Wildlife tracking and behaviour analysis represent a critical field in ecological research, contributing to the conservation of diverse animal species. In recent years, the integration of Artificial Intelligence (AI) techniques has greatly enhanced our ability to extract valuable insights from the wealth of data collected through tracking devices, camera traps, and acoustic sensors. AI techniques such as Normalized Difference Vegetation Index (NDVI) can be helpful in inferring changes in animal populations through the number of plants in an area, thereby quantifying a given species in the area, acoustic sensors can be used to monitor animal sounds and vocalizations, while heat signature detection in thermal imagery can be used to track animal, and image recognition can be used to identify individual animals based on their unique physical characteristics such as stripes or spots. As briefly discussed, AI techniques are integrated in numerous aspects in wildlife tracking and behaviour analysis. This project will discuss on existing object recognition techniques more in-depth with a literature review. The author will also perform a literature survey and performance evaluation to decide which is the most ideal Machine Learning approach.\n",
        "\n",
        "This study introduces a novel deep learning-based facial recognition pipeline (BearID) for identifying individual brown bears (Ursus arctos) in wild populations. Traditional facial recognition methods often struggle with species that lack unique markings, such as brown bears, hindering research and conservation efforts. BearID addresses this challenge by employing a two-stage approach: first, it detects bear faces in images using a trained face detector (RetinaFace), and second, it generates embeddings for each detected face using a trained face encoder (bearembed). These embeddings are then used to identify individual bears based on a trained classifier (bearsvm). BearID was evaluated using images of 100 known American black bears (Ursus americanus) from a captive population. The face detector achieved an average precision of 97.7%, demonstrating its ability to accurately detect bear faces in a variety of poses. The bearembed encoder achieved an identification accuracy of 84% when matching new images of known individuals and 71% when assigning bears as training or testing individuals. The bearsvm classifier achieved an identification accuracy of 83.9%. This pipeline has the potential to address a broad spectrum of ecological questions that require individual identification, from fine-scale behaviour to landscape-level population assessments. It also has potential for use in conservation practice, such as identifying problem individuals in human-wildlife conflicts and evaluating intrapopulation variation in the efficacy of conservation strategies. (Clapham et al., 2020)\n",
        "\n",
        "This study explores the application of deep learning to automated bird counting for regional bird distribution mapping. A deep learning model was trained on a large dataset of bird images, demonstrating high accuracy in identifying and counting birds across diverse species and habitats. This model was then applied to analyse images from various locations within the study area, generating detailed bird distribution maps. The maps revealed significant variations in bird abundance across the region, providing valuable insights into bird population dynamics and habitat preferences. This automated bird counting method offers several advantages over traditional manual counting techniques, including its cost-effectiveness, efficiency, and ability to generate large-scale bird distribution maps. The findings underscore the potential of deep learning to revolutionize bird monitoring and conservation efforts, enabling informed decision-making for the protection of critical habitats and the persistence of bird populations. (Akçay et al., 2020)\n",
        "\n",
        "This study presents DeepBhvTracking, a novel behaviour tracking method for laboratory animals based on deep learning. Traditional animal tracking methods often rely on manual observation or specialized equipment, which can be time-consuming, labour-intensive, and expensive. DeepBhvTracking addresses these limitations by utilizing deep learning algorithms to automatically track animal movements and behaviours from video recordings. This is done by employing a two-stage approach: first, it detects and segments animals in the video frames using a YOLO model, and second, it tracks individual animals across frames using a Long Short-Term Memory (LSTM) network. The LSTM network is trained on a dataset of animal tracking annotations, enabling it to learn the temporal patterns of animal movements and maintain consistent tracking across frames. DeepBhvTracking was evaluated on a dataset of video recordings of mice and rats performing various behaviours, such as grooming, rearing, and locomotion. The results demonstrated that DeepBhvTracking achieved high accuracy in tracking animal movements and behaviours, outperforming traditional tracking methods. Overall, DeepBhvTracking represents a significant advancement in animal behaviour tracking, offering a powerful tool for researchers studying animal behaviour in a variety of laboratory settings. Its potential to automate and streamline behavioural data collection could significantly accelerate the pace of behavioural research and lead to new discoveries in animal behaviour and cognition. (Sun et al., 2021)\n",
        "\n",
        "This study introduces a large-scale, multi-class, high-quality open-source dataset (WAID - Wildlife Aerial Images from Drone) for wildlife detection with drones. It contains 14,375 UAV aerial images from different environmental conditions, covering six wildlife species and multiple habitat types. The dataset can be used to study the effectiveness of different wildlife detection algorithms and has the potential to improve the accuracy and efficiency of wildlife monitoring using drones. The paper also experimented with state-of-the-art algorithms which includes Faster R-CNN, SSD, YOLOv5, YOLOv7, and YOLOv8 on the WAID dataset. Overall, the YOLO model exhibits the best performance, with YOLOv5, YOLOv7, and YOLOv8 achieving mAP scores of 95.6%, 97.4%, and 95.8% respectively. Additionally, our proposed SE-YOLO model achieved the highest mAP of 98.3%. Although there is not a significant difference between the Faster R-CNN and SSD models overall, Faster R-CNN slightly outperforms the SSD model in accuracy while the SSD model outperforms Faster R-CNN in speed. In conclusion, this study on the small target detection problem faced by wildlife monitoring drones. To solve this problem, we propose the SE-YOLO method, which dramatically improves wildlife detection from aerial image data by integrating the SEAttention module into YOLOv7. In particular, SE-YOLO can accurately detect minimal targets with a size of 20–50 pixels. (Mou et al., 2023)\n",
        "\n",
        "\n",
        "<table>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\"><b>Citation</b></td>\n",
        "    <td style=\"width:300px;\"><b>Technologies</b></td>\n",
        "    <td style=\"width:600px;\"><b>Contribution</b></td>\n",
        "    <td style=\"width:600px;\"><b>Limitation</b></td>\n",
        "  </tr>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\">(Clapham et al., 2020)</td>\n",
        "    <td style=\"width:300px;\">Deep CNN</td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Created an open-source application called BearID that implements a novel deep learning approach for facial recognition in brown bears, a species that lacks unique markings.</li>\n",
        "          <li>Achieved an average precision of 0.98 for facial detection and an accuracy of 83.9% for individual classification.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>The study only included a limited number of individuals (132) from two populations.</li>\n",
        "          <li>The performance of the deep learning approach is affected by factors such as image quality, lighting conditions, and pose of the bear.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\">(Akçay et al., 2020)</td>\n",
        "    <td style=\"width:300px;\">Faster R-CNN and SSD</td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Developed an image-based bird population counting and mapping approach using deep learning.</li>\n",
        "          <li>Established a new dataset of wild bird photographs taken regularly in various environments.</li>\n",
        "          <li>Created spatial bird order distribution and species diversity maps of Turkey.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>The study did not consider factors such as bird behaviour and interactions, which could influence bird distribution patterns.</li>\n",
        "          <li>The study focused on a limited number of bird species (38) within Turkey.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\">(Sun et al., 2021)</td>\n",
        "    <td style=\"width:300px;\">YOLO (You Only Look Once) algorithm</td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Proposed a novel behaviour tracking method, that combines a deep learning algorithm (YOLO) and a background subtraction algorithm.</li>\n",
        "          <li>Developed a detector training workflow that combines a pretrained deep-learning neural network and YOLO.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Computationally very demanding to run.</li>\n",
        "          <li>DeepBhvTracking is not as accurate as manual tracking in situations where the animal is occluded or there are multiple animals in the scene.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr style=\"height:50px;\">\n",
        "    <td style=\"width:150px;\">(Mou et al., 2023)</td>\n",
        "    <td style=\"width:300px;\">Integrating the SEAttention module into YOLOv7 (SE-YOLO)</td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>Provides a large-scale, multi-class, high-quality dataset for wildlife detection with drones.</li>\n",
        "          <li>Conducts an algorithm detection comparison experiment to compare and evaluate different types of advanced algorithms.</li>\n",
        "          <li>Provides a new method, data, and inspiration to the field of wildlife monitoring by UAVs.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "    <td style=\"width:600px;\">\n",
        "      <ul>\n",
        "          <li>The dataset is limited to UAV aerial images and due to this, it is relatively small compared to other object detection datasets.</li>\n",
        "          <li>The dataset is limited to six wildlife species which means that the dataset may not be suitable for detecting other wildlife species or in other habitat types.</li>\n",
        "      </ul>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxa53D8u5HN-"
      },
      "source": [
        "***\n",
        "# **Part B - Comparison and Evaluation of AI Techniques**\n",
        "\n",
        "## **1.\tDeep Convolutional Neural Network**\n",
        "\n",
        "## **2. Faster Region-based Convolutional Neural Network**\n",
        "\n",
        "## **3. Squeeze and Excitation YOLOv7**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQXk0hia5W4F"
      },
      "source": [
        "***\n",
        "# **Part C - Implementation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7vD3ny55gxN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2qSGrVv5hHN"
      },
      "source": [
        "***\n",
        "# **Part D - Testing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BktdCr215ngl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GRXKlkk5n1N"
      },
      "source": [
        "***\n",
        "# **Part E - Evaluations of Results**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD2Pgtpa5sh8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvsmGdf_423a"
      },
      "source": [
        "***\n",
        "# **References**\n",
        "\n",
        "* Akçay, H.G. et al. (2020). Automated Bird Counting with Deep Learning for Regional Bird Distribution Mapping. Animals, 10 (7), 1207. Available from https://doi.org/10.3390/ani10071207.\n",
        "\n",
        "* Clapham, M. et al. (2020). Automated facial recognition for wildlife that lack unique markings: A deep learning approach for brown bears. Ecology and Evolution, 10 (23), 12883–12892. Available from https://doi.org/10.1002/ece3.6840.\n",
        "\n",
        "* Mou, C. et al. (2023). WAID: A Large-Scale Dataset for Wildlife Detection with Drones. Applied Sciences, 13 (18), 10397. Available from https://doi.org/10.3390/app131810397.\n",
        "\n",
        "* Sun, G. et al. (2021). DeepBhvTracking: A Novel Behavior Tracking Method for Laboratory Animals Based on Deep Learning. Frontiers in Behavioral Neuroscience, 15. Available from https://www.frontiersin.org/articles/10.3389/fnbeh.2021.750894 [Accessed 20 November 2023].\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
