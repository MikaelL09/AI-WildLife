{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils \n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1204 images belonging to 4 classes.\n",
      "Found 300 images belonging to 4 classes.\n",
      "Class labels: ['buffalo', 'elephant', 'rhino', 'zebra']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import dataset\n",
    "data_path = './Data/CNN_datasets/new'\n",
    "\n",
    "# Use ImageDataGenerator for loading and augmenting the data\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Set the validation split\n",
    ")\n",
    "\n",
    "# Create train and validation generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Set to 'training' for the train split\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Set to 'validation' for the validation split\n",
    ")\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Display the class labels\n",
    "print(\"Class labels:\", class_labels)\n",
    "\n",
    "# Optional: Convert class indices to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_generator.labels, num_classes=len(class_labels))\n",
    "val_labels = tf.keras.utils.to_categorical(val_generator.labels, num_classes=len(class_labels))\n",
    "\n",
    "# Optional: Inspect the first batch of images and labels\n",
    "sample_images, sample_labels = next(train_generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Load Faster R-CNN model with MobileNetV2 backbone\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create Faster R-CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(len(class_labels), activation='softmax')  # Adjust output classes based on your dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 56s 1s/step - loss: 0.5946 - accuracy: 0.7857 - val_loss: 0.2432 - val_accuracy: 0.9200\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 41s 1s/step - loss: 0.2046 - accuracy: 0.9352 - val_loss: 0.1667 - val_accuracy: 0.9433\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 41s 1s/step - loss: 0.1620 - accuracy: 0.9510 - val_loss: 0.1837 - val_accuracy: 0.9433\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 41s 1s/step - loss: 0.1339 - accuracy: 0.9568 - val_loss: 0.1170 - val_accuracy: 0.9667\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 40s 1s/step - loss: 0.1131 - accuracy: 0.9693 - val_loss: 0.1170 - val_accuracy: 0.9767\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 40s 1s/step - loss: 0.0928 - accuracy: 0.9743 - val_loss: 0.0959 - val_accuracy: 0.9633\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 40s 1s/step - loss: 0.1006 - accuracy: 0.9684 - val_loss: 0.1191 - val_accuracy: 0.9600\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 40s 1s/step - loss: 0.0849 - accuracy: 0.9743 - val_loss: 0.1086 - val_accuracy: 0.9567\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 40s 1s/step - loss: 0.0868 - accuracy: 0.9767 - val_loss: 0.1133 - val_accuracy: 0.9500\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 40s 1s/step - loss: 0.0696 - accuracy: 0.9834 - val_loss: 0.1341 - val_accuracy: 0.9567\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have already prepared train_generator and val_generator\n",
    "history = model.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 7s 630ms/step - loss: 0.1167 - accuracy: 0.9567\n",
      "Test Loss: 0.11668369174003601, Test Accuracy: 0.9566666483879089\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Evaluate using metrics\n",
    "# Assuming you have already prepared test_generator\n",
    "test_loss, test_accuracy = model.evaluate(val_generator)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save the model\n",
    "model.save('faster_rcnn_gg1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002ADB157AA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.03886882 0.41592526 0.01804311 0.5271628 ]]\n",
      "zebra\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Load the model\n",
    "model = tf.keras.models.load_model('faster_rcnn_gg1.h5')\n",
    "\n",
    "# Step 12: Make predictions\n",
    "image_path = './Data/CNN_datasets/final_testing/1.jpeg'\n",
    "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "input_arr = keras.preprocessing.image.img_to_array(image)\n",
    "input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "predictions = model.predict(input_arr)\n",
    "print(predictions)\n",
    "print(class_labels[np.argmax(predictions[0])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visually present the results\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('faster_rcnn_gg1.h5')\n",
    "\n",
    "# Load the image\n",
    "image_path = './Data/CNN_datasets/final_testing/1.jpeg'\n",
    "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(input_arr)\n",
    "class_label = class_labels[np.argmax(predictions[0])]\n",
    "\n",
    "# Convert image to NumPy array\n",
    "image_np = cv2.imread(image_path)\n",
    "\n",
    "# Get bounding box coordinates\n",
    "ymin, xmin, ymax, xmax = predictions[0]\n",
    "\n",
    "# Convert normalized coordinates to pixel values\n",
    "height, width, _ = image_np.shape\n",
    "xmin = int(xmin * width)\n",
    "xmax = int(xmax * width)\n",
    "ymin = int(ymin * height)\n",
    "ymax = int(ymax * height)\n",
    "\n",
    "# Draw bounding box on the image\n",
    "color = (0, 255, 0)  # Green color for the bounding box\n",
    "thickness = 2\n",
    "cv2.rectangle(image_np, (xmin, ymin), (xmax, ymax), color, thickness)\n",
    "\n",
    "# Display the image with bounding box\n",
    "cv2.putText(image_np, class_label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "cv2.imshow('Object Detection', image_np)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
